# PySpark Data Processing and Machine Learning Assignment
## Yahlly Schein

## Project Overview
This project leverages PySpark for distributed data processing and machine learning, executed on the Databricks platform.  
The goal is to efficiently process and analyze large datasets while adhering to specific constraints, such as avoiding the use of Pandas and PySpark SQL.   
Additionally, the project includes a section where PySpark's MLlib is utilized for machine learning tasks.

**Executed on Databricks:** The entire project is run on Databricks, taking advantage of its managed Spark environment for scalability and performance optimization.

## Key Sections
**Data Loading:** Datasets are loaded using PySpark's DataFrame API, and initial exploratory data analysis is performed to understand the structure and content of the data.  
**Data Transformation:** Various data transformations are applied to clean, filter, and aggregate the data according to the specified requirements.  
**Machine Learning with PySpark MLlib:** A section is dedicated to implementing machine learning models using PySpark MLlib, demonstrating the application of distributed machine learning techniques.  
**Analysis and Results:** The notebook includes analysis steps to derive insights from the data, followed by visualizations and final outputs that summarize the results.  
**Conclusion:** The final section wraps up the findings and reflects on the efficiency and scalability of PySpark for handling large datasets, particularly in a Databricks environment.
